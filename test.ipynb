{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nasdata4/pei4/anaconda3/envs/mye_venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import SequentialSampler\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data, Dataset, Batch\n",
    "from torch_geometric.nn import GCNConv, GINConv, TopKPooling\n",
    "from torch_geometric.nn import (\n",
    "    global_mean_pool as gap,\n",
    "    global_max_pool as gmp,\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error as MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90server: TITAN Xp - cuda(3) v10.2 is available\n",
      "Torch version: 1.9.1+cu102\n",
      "Count of using GPUs: 8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GPU_NUM = 3\n",
    "torch.cuda.set_device(GPU_NUM)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\n",
    "        f\"90server: {torch.cuda.get_device_name()} - cuda({torch.cuda.current_device()}) v{torch.version.cuda} is available\"\n",
    "    )\n",
    "    print(f\"Torch version: {torch.__version__}\")\n",
    "    print(f\"Count of using GPUs:\", torch.cuda.device_count())\n",
    "    print()\n",
    "    torch.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "else:\n",
    "    print(\"Can not use GPU device!\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH        = \"/nasdata4/pei4/feature_selection_gnn/experiments\"\n",
    "suffix      = 'Y'\n",
    "saving_path = PATH + \"/experiment_\" + suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = '/nasdata4/pei4/feature_selection_gnn/sample_data/'\n",
    "\n",
    "# # 데이터 파일 읽어오기\n",
    "# subjects_list = open(data_path + 'subjects_list.txt', 'r')\n",
    "# subjects_list = subjects_list.read()\n",
    "# subjects_list = subjects_list.split('\\n')\n",
    "\n",
    "# subjects_diag = open(data_path + 'subjects_diagnosis.txt', 'r')\n",
    "# subjects_diag = subjects_diag.read()\n",
    "# subjects_diag = subjects_diag.split('\\n')\n",
    "\n",
    "# snp_Additive    = pd.read_csv(data_path + 'Data01_SNPs_Additive.csv')\n",
    "# snp_GENEgroups  = pd.read_csv(data_path + 'Data02_SNPs_GENEgroups.csv')\n",
    "# subcor_volumes  = pd.read_csv(data_path + 'Data04_Subcortical_volumes_ICVcorrected.csv')\n",
    "# normal_subcor_volumes  = pd.read_csv(data_path + 'Data04_Subcortical_volumes_ICVcorrected_Normalized.csv')\n",
    "\n",
    "# # 데이터 합치기\n",
    "# smaple_data_features = []\n",
    "# for sbj, label in zip(subjects_list, subjects_diag):\n",
    "#     edge_feature5       = np.loadtxt(f'{data_path}edge_index5/{sbj}_edge_index.txt', dtype=int)\n",
    "#     edge_feature10      = np.loadtxt(f'{data_path}edge_index10/{sbj}_edge_index.txt', dtype=int)\n",
    "#     edge_feature15      = np.loadtxt(f'{data_path}edge_index15/{sbj}_edge_index.txt', dtype=int)\n",
    "#     edge_feature20      = np.loadtxt(f'{data_path}edge_index20/{sbj}_edge_index.txt', dtype=int)\n",
    "#     node_feature_pcc    = np.loadtxt(f'{data_path}x_partialCC/pcc_ROISignals_{sbj}.txt', delimiter=\",\")\n",
    "#     node_feature_bold   = np.loadtxt(f'{data_path}x_roi_signal/ROISignals_{sbj}.txt')\n",
    "\n",
    "#     graph_label = int(label[-1])\n",
    "\n",
    "#     temp = snp_Additive.loc[snp_Additive['IID'] == sbj]\n",
    "#     snp_data = temp.get(temp.columns[6:]).to_numpy()    # 3001개의 SNP값만 쏙 읽어오기\n",
    "\n",
    "#     temp = subcor_volumes.loc[subcor_volumes['IID'] == sbj]\n",
    "#     subcor_data = temp.get(temp.columns[1:]).to_numpy(dtype=float)  # 8개의 볼륨값만 쏙 읽어오기\n",
    "\n",
    "#     temp = normal_subcor_volumes.loc[normal_subcor_volumes['IID'] == sbj]\n",
    "#     normal_subcor_data = temp.get(temp.columns[1:]).to_numpy(dtype=float)  # 8개의 볼륨값만 쏙 읽어오기\n",
    "\n",
    "#     smaple_data_features.append([sbj,                   # [0]\n",
    "#                                  node_feature_pcc,      # [1]\n",
    "#                                  node_feature_bold,     # [2]\n",
    "#                                  edge_feature5,         # [3]\n",
    "#                                  edge_feature10,        # [4]\n",
    "#                                  edge_feature15,        # [5]\n",
    "#                                  edge_feature20,        # [6]\n",
    "#                                  graph_label,           # [7]\n",
    "#                                  snp_data,              # [8]\n",
    "#                                  subcor_data,           # [9]\n",
    "#                                  normal_subcor_data     # [10]\n",
    "#                                  ])\n",
    "\n",
    "# # 데이터 저장\n",
    "# smaple_data_features = np.asarray(smaple_data_features, dtype=object)\n",
    "# np.save(data_path + 'smaple_data_features.npy', smaple_data_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_dataset(Dataset):\n",
    "    def __init__(self, data_path, node_feature_type=\"pcor\", edge_sparse_type=5, snp_data_type=\"real\", qt_data_type='norm', num_fold=5):\n",
    "        super(my_dataset, self).__init__()\n",
    "        self.node_feature_type  = node_feature_type\n",
    "        self.edge_sparse_type   = edge_sparse_type\n",
    "        self.snp_data_type      = snp_data_type\n",
    "        self.qt_data_type       = qt_data_type\n",
    "\n",
    "        snp_Additive    = pd.read_csv(data_path + 'Data01_SNPs_Additive.csv')   # snp additive 정보 읽어오기\n",
    "        snp_GENEgroups  = pd.read_csv(data_path + 'Data02_SNPs_GENEgroups.csv') # snp gene group 정보 읽어오기\n",
    "        add_snp_list    = snp_Additive.columns[6:].to_list()                    # csv파일에서 FID, IID, PAT, MAT, SEX, PHENOTYPE 제외한 SNP정보만 추출\n",
    "        gene_group      = dict(zip(list(snp_GENEgroups.SNPID), list(snp_GENEgroups.GENEgroup))) # {SNP명 : group 번호} 생성\n",
    "        # gene_group_list = list(snp_GENEgroups.SNPID)    # group 번호 정보를 담은 list data 생성\n",
    "\n",
    "        self.snp_group = []\n",
    "        for snp_name in add_snp_list:\n",
    "            self.snp_group.append(gene_group[snp_name[:-2]])    # SNP명을 key로 받는 dictionary를 이용해서 add_snp_list의 SNP명 순서로 group 번호 할당\n",
    "        self.snp_group_data = torch.tensor(self.snp_group, dtype=torch.long)\n",
    "\n",
    "        self.my_Data = []\n",
    "        self.num_sbj    = 157\n",
    "        self.num_QT     = 8\n",
    "        self.num_snp    = 3001\n",
    "\n",
    "        smaple_data_features = np.load(data_path + 'smaple_data_features.npy', allow_pickle=True)\n",
    "        for subject_data in smaple_data_features:\n",
    "            if node_feature_type == \"pcor\":\n",
    "                node_feature = subject_data[1]\n",
    "            else:   # \"bold\"\n",
    "                node_feature = subject_data[2]\n",
    "\n",
    "            if edge_sparse_type == 5:\n",
    "                edge_idx = subject_data[3]\n",
    "            elif edge_sparse_type == 10:\n",
    "                edge_idx = subject_data[4]\n",
    "            elif edge_sparse_type == 15:\n",
    "                edge_idx = subject_data[5]\n",
    "            else:\n",
    "                edge_idx = subject_data[6]\n",
    "\n",
    "            graph_label = subject_data[7]\n",
    "\n",
    "            if self.snp_data_type == \"real\":\n",
    "                snp_data = subject_data[8]\n",
    "            else:\n",
    "                snp_data = torch.randint(0, 3, size=(157, 3001)).to(device=device)\n",
    "                torch.save(snp_data, data_path + \"/fake_snp_data.pt\")\n",
    "\n",
    "            if self.qt_data_type == \"raw\":\n",
    "                t1_measure  = subject_data[9]\n",
    "            else:\n",
    "                t1_measure  = subject_data[10]\n",
    "\n",
    "            # numpy to tensor\n",
    "            node_feature    = torch.Tensor(node_feature).transpose(0, 1)\n",
    "            graph_label     = torch.tensor(graph_label, dtype=torch.long)\n",
    "            edge_flag       = generate_edge_flag(116, edge_idx)\n",
    "            edge_idx        = torch.Tensor(edge_idx).transpose(0, 1)\n",
    "            # edge_flag       = generate_edge_flag(116, edge_idx)\n",
    "            edge_flag       = torch.Tensor(edge_flag).to(device=device)\n",
    "            edge_attr       = torch.randint(0, 3, size=(1, 672)).to(device=device)\n",
    "            edge_attr       = edge_attr.squeeze(0)\n",
    "            # print(\"edge_idx: \", edge_idx.shape)\n",
    "            # print(\"edge_attr: \", edge_attr.shape)\n",
    "            fc_data         = Data(x=node_feature,\n",
    "                                   edge_index=edge_idx.long(),\n",
    "                                   y=graph_label,\n",
    "                                   edge_attr=edge_attr,\n",
    "                                   edge_flag=edge_flag.long())\n",
    "\n",
    "            snp_data    = torch.Tensor(snp_data).to(device=device)\n",
    "            t1_measure  = torch.Tensor(t1_measure).to(device=device)\n",
    "            snp_data    = torch.squeeze(snp_data)\n",
    "            t1_measure  = torch.squeeze(t1_measure)\n",
    "\n",
    "            self.my_Data.append([fc_data, snp_data, t1_measure])\n",
    "\n",
    "        rand_state = 42\n",
    "        self.num_fold = num_fold\n",
    "        skf = StratifiedKFold(n_splits=self.num_fold, shuffle=True, random_state=rand_state)\n",
    "\n",
    "        y_idx = []\n",
    "        for i in range(self.num_sbj):\n",
    "            y_idx.append(self.my_Data[i][0].y.item())\n",
    "\n",
    "        self.train_dataset  = []\n",
    "        self.test_dataset   = []\n",
    "        self.y_train = []\n",
    "        for train_idx, test_idx in skf.split(range(self.num_sbj), y_idx):\n",
    "            for i in train_idx:\n",
    "                self.train_dataset.append([self.my_Data[i][0], self.my_Data[i][1], self.my_Data[i][2]])\n",
    "                self.y_train.append(self.my_Data[i][0].y.item())\n",
    "            for j in test_idx:\n",
    "                self.test_dataset.append([self.my_Data[j][0], self.my_Data[j][1], self.my_Data[j][2]])\n",
    "            break\n",
    "\n",
    "    # 인덱스에 해당되는 데이터를 tensor 형태로 반환\n",
    "    def __getitem__(self, idx):\n",
    "        fc_map = self.my_Data[idx][0]\n",
    "        minor_allele_cnt = self.my_Data[idx][1]\n",
    "        t1_meausre = self.my_Data[idx][2]\n",
    "\n",
    "        return fc_map, minor_allele_cnt, t1_meausre\n",
    "\n",
    "    # 데이터 총 개수 반환\n",
    "    def __len__(self):\n",
    "        return len(self.num_sbj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_sparse import SparseTensor\n",
    "from torch_geometric.typing import Adj, Size\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import Parameter, Linear\n",
    "from torch.nn import functional as F\n",
    "from torch_scatter import scatter_add\n",
    "from torch_sparse import SparseTensor, set_diag\n",
    "from torch_geometric.nn import global_add_pool, global_mean_pool\n",
    "from torch_sparse import SparseTensor, matmul, fill_diag, sum as sparsesum, mul\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops, softmax\n",
    "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "from typing import Tuple, Optional\n",
    "from torch import Tensor\n",
    "from sklearn import metrics\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch_geometric.data import DataLoader\n",
    "import nni\n",
    "import os\n",
    "import random\n",
    "from typing import List\n",
    "\n",
    "def maybe_num_nodes(edge_index, num_nodes=None):\n",
    "    if num_nodes is not None:\n",
    "        return num_nodes\n",
    "    elif isinstance(edge_index, Tensor):\n",
    "        return int(edge_index.max()) + 1\n",
    "    else:\n",
    "        return max(edge_index.size(0), edge_index.size(1))\n",
    "\n",
    "def _remove_self_loops(edge_index, edge_attr: torch.Tensor, edge_flags: torch.Tensor):\n",
    "    mask = edge_index[0] != edge_index[1]\n",
    "    edge_index = edge_index[:, mask]\n",
    "    return edge_index, edge_attr[mask], edge_flags[mask]\n",
    "\n",
    "def _add_self_loops(edge_index, edge_weight: Optional[torch.Tensor] = None,\n",
    "                    edge_flags: Optional[torch.Tensor] = None,\n",
    "                    fill_value: float = 1., num_nodes: Optional[int] = None):\n",
    "    N = maybe_num_nodes(edge_index, num_nodes)\n",
    "\n",
    "    loop_index = torch.arange(0, N, dtype=torch.long, device=edge_index.device)\n",
    "    loop_index = loop_index.unsqueeze(0).repeat(2, 1)\n",
    "\n",
    "    if edge_weight is not None:\n",
    "        assert edge_weight.numel() == edge_index.size(1)\n",
    "        loop_weight = edge_weight.new_full((N,), fill_value)\n",
    "        edge_weight = torch.cat([edge_weight, loop_weight], dim=0)\n",
    "    if edge_flags is not None:\n",
    "        assert edge_flags.numel() == edge_index.size(1)\n",
    "        loop_weight = edge_flags.new_full((N,), fill_value)\n",
    "        edge_flags = torch.cat([edge_flags, loop_weight], dim=0)\n",
    "\n",
    "    edge_index = torch.cat([edge_index, loop_index], dim=1)\n",
    "\n",
    "    return edge_index, edge_weight, edge_flags\n",
    "\n",
    "def generate_edge_flag(num_nodes, edge_index):\n",
    "    \n",
    "    edge_flag = np.full((num_nodes**2, ), False)\n",
    "    # print(edge_index.shape)\n",
    "    # print(edge_index)\n",
    "    for i in range(edge_index.shape[1]):\n",
    "        source = edge_index[0][i]\n",
    "        # print(\"==>> source: \", source)\n",
    "        target = edge_index[1][i]\n",
    "        # print(\"==>> target: \", target)\n",
    "        new_index = source * num_nodes + target\n",
    "        # print(\"==>> new_index: \", new_index.shape)\n",
    "        edge_flag[int(new_index)] = True\n",
    "    # print(edge_flag.shape)\n",
    "    return edge_flag\n",
    "\n",
    "class ModifiedMessagePassing(MessagePassing):\n",
    "    def message_and_aggregate(self, adj_t: SparseTensor) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def propagate(self, edge_index: Adj, pruned_edge_mask, size: Size = None, **kwargs):\n",
    "        size = self.__check_input__(edge_index, size)\n",
    "\n",
    "        # Run \"fused\" message and aggregation (if applicable).\n",
    "        if (isinstance(edge_index, SparseTensor) and self.fuse\n",
    "                and not self.__explain__):\n",
    "            coll_dict = self.__collect__(self.__fused_user_args__, edge_index,\n",
    "                                         size, kwargs)\n",
    "\n",
    "            msg_aggr_kwargs = self.inspector.distribute(\n",
    "                'message_and_aggregate', coll_dict)\n",
    "            out = self.message_and_aggregate(edge_index, **msg_aggr_kwargs)\n",
    "\n",
    "            update_kwargs = self.inspector.distribute('update', coll_dict)\n",
    "            return self.update(out, **update_kwargs)\n",
    "\n",
    "        # Otherwise, run both functions in separation.\n",
    "        elif isinstance(edge_index, Tensor) or not self.fuse:\n",
    "            coll_dict = self.__collect__(self.__user_args__, edge_index, size,\n",
    "                                         kwargs)\n",
    "            msg_kwargs = self.inspector.distribute('message', coll_dict)\n",
    "            out = self.message(**msg_kwargs)\n",
    "\n",
    "            # For `GNNExplainer`, we require a separate message and aggregate\n",
    "            # procedure since this allows us to inject the `edge_mask` into the\n",
    "            # message passing computation scheme.\n",
    "            if self.__explain__:\n",
    "                # edge_mask = self.__edge_mask__.sigmoid()\n",
    "                assert out.size(self.node_dim) == pruned_edge_mask.size(0)\n",
    "                out = out * pruned_edge_mask.view([-1] + [1] * (out.dim() - 1))\n",
    "\n",
    "            aggr_kwargs = self.inspector.distribute('aggregate', coll_dict)\n",
    "            out = self.aggregate(out, **aggr_kwargs)\n",
    "\n",
    "            update_kwargs = self.inspector.distribute('update', coll_dict)\n",
    "            return self.update(out, **update_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcn_norm(edge_index, edge_flag, edge_attr=None, num_nodes=None, improved=False,\n",
    "             do_add_self_loops=True, dtype=None):\n",
    "    fill_value = 2. if improved else 1.\n",
    "\n",
    "    if isinstance(edge_index, SparseTensor):\n",
    "        adj_t = edge_index\n",
    "        if not adj_t.has_value():\n",
    "            adj_t = adj_t.fill_value(1., dtype=dtype)\n",
    "        if do_add_self_loops:\n",
    "            adj_t = fill_diag(adj_t, fill_value)\n",
    "        deg = sparsesum(adj_t, dim=1)\n",
    "        deg_inv_sqrt = deg.pow_(-0.5)\n",
    "        deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0.)\n",
    "        adj_t = mul(adj_t, deg_inv_sqrt.view(-1, 1))\n",
    "        adj_t = mul(adj_t, deg_inv_sqrt.view(1, -1))\n",
    "        return adj_t\n",
    "    else:\n",
    "        num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "        if edge_attr is None:\n",
    "            edge_attr = torch.ones((edge_index.size(1),), dtype=dtype,\n",
    "                                   device=edge_index.device)\n",
    "        if do_add_self_loops:\n",
    "            if isinstance(edge_index, Tensor):\n",
    "                if isinstance(edge_flag, Tensor):\n",
    "                    edge_index, edge_attr, edge_flag = _remove_self_loops(edge_index, edge_attr, edge_flag)\n",
    "                    edge_index, edge_attr, edge_flag = _add_self_loops(edge_index, edge_attr, edge_flag,\n",
    "                                                                       num_nodes=num_nodes)\n",
    "                else:\n",
    "                    edge_index, edge_attr = remove_self_loops(edge_index, edge_attr)\n",
    "                    edge_index, edge_attr = add_self_loops(edge_index, edge_attr, num_nodes=num_nodes)\n",
    "            elif isinstance(edge_index, SparseTensor):\n",
    "                edge_index = set_diag(edge_index)\n",
    "\n",
    "        row, col = edge_index[0], edge_index[1]\n",
    "        deg = scatter_add(edge_attr, col, dim=0, dim_size=num_nodes)\n",
    "        deg_inv_sqrt = deg.pow_(-0.5)\n",
    "        deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0)\n",
    "        return edge_index, deg_inv_sqrt[row] * edge_attr * deg_inv_sqrt[col], edge_flag\n",
    "\n",
    "\n",
    "class MPConv(ModifiedMessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, improved: bool = False, cached: bool = False,\n",
    "                 add_self_loops: bool = True, normalize: bool = True, bias: bool = True):\n",
    "        super(MPConv, self).__init__(aggr='add')\n",
    "\n",
    "        self.improved = improved\n",
    "        self.cached = cached\n",
    "        self.add_self_loops = add_self_loops\n",
    "        self.normalize = normalize\n",
    "        self._cached_edge_index = None\n",
    "        self._cached_adj_t = None\n",
    "        self.__explain__ = False\n",
    "\n",
    "        self.weight = Parameter(torch.Tensor(in_channels, out_channels))\n",
    "\n",
    "        self.lin = torch.nn.Linear(out_channels*2 + 1, out_channels)\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot(self.weight)\n",
    "        zeros(self.bias)\n",
    "        self._cached_edge_index = None\n",
    "        self._cached_adj_t = None\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, edge_flag):\n",
    "        if self.normalize:\n",
    "            if isinstance(edge_index, Tensor):\n",
    "                cache = self._cached_edge_index\n",
    "                if cache is None:\n",
    "                    edge_index, edge_weight, edge_flag = gcn_norm(  # yapf: disable\n",
    "                        edge_index, edge_flag, edge_attr, x.size(self.node_dim),\n",
    "                        self.improved, self.add_self_loops)\n",
    "                    if self.cached:\n",
    "                        self._cached_edge_index = (edge_index, edge_weight, edge_flag)\n",
    "                else:\n",
    "                    edge_index, edge_weight, edge_flag = cache[0], cache[1], cache[2]\n",
    "\n",
    "            elif isinstance(edge_index, SparseTensor):\n",
    "                cache = self._cached_adj_t\n",
    "                if cache is None:\n",
    "                    edge_index = gcn_norm(  # yapf: disable\n",
    "                        edge_index, edge_attr, x.size(self.node_dim),\n",
    "                        self.improved, self.add_self_loops)\n",
    "                    if self.cached:\n",
    "                        self._cached_adj_t = edge_index\n",
    "                else:\n",
    "                    edge_index = cache\n",
    "\n",
    "        x = x @ self.weight\n",
    "\n",
    "        # propagate_type: (x: Tensor, edge_weight: OptTensor)\n",
    "        out = self.propagate(edge_index, edge_flag, x=x, edge_attr=edge_weight)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out += self.bias\n",
    "\n",
    "        return out\n",
    "\n",
    "    def message(self, x_i, x_j, edge_attr):\n",
    "        msg = torch.cat([x_i, x_j, edge_attr.view(-1, 1)], dim=1)\n",
    "        return self.lin(msg)\n",
    "\n",
    "\n",
    "class IBGConv(torch.nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(IBGConv, self).__init__()\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "\n",
    "        hidden_dim = 16\n",
    "        num_layers = 2\n",
    "        self.pooling = 'sum'\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            if i == 0:\n",
    "                conv = MPConv(input_dim, hidden_dim)\n",
    "            elif i != num_layers - 1:\n",
    "                conv = MPConv(hidden_dim, hidden_dim)\n",
    "            else:\n",
    "                conv = MPConv(hidden_dim, num_classes)\n",
    "            self.convs.append(conv)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, edge_flag, batch):\n",
    "        z = x\n",
    "        edge_attr[edge_attr < 0] = - edge_attr[edge_attr < 0]\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            z = conv(z, edge_index, edge_attr, edge_flag)\n",
    "            if i != len(self.convs) - 1:\n",
    "                z = F.relu(z)  # [N * M, F]\n",
    "                z = F.dropout(z, training=self.training)\n",
    "            if self.pooling == 'sum':\n",
    "                g = global_add_pool(z, batch)  # [N, F]\n",
    "            elif self.pooling == 'mean':\n",
    "                g = global_mean_pool(z, batch)  # [N, F]\n",
    "            else:\n",
    "                raise NotImplementedError('Pooling method not implemented')\n",
    "\n",
    "        return F.log_softmax(g, dim=-1)\n",
    "\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, activation, n_classes=0):\n",
    "        super(MLP, self).__init__()\n",
    "        self.net = []\n",
    "        self.net.append(torch.nn.Linear(input_dim, hidden_dim))\n",
    "        self.net.append(activation())\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.net.append(torch.nn.Linear(hidden_dim, hidden_dim))\n",
    "            self.net.append(activation())\n",
    "        self.net = torch.nn.Sequential(*self.net)\n",
    "        self.shortcut = torch.nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        if n_classes != 0:\n",
    "            self.classifier = torch.nn.Linear(hidden_dim, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x) + self.shortcut(x)\n",
    "        if hasattr(self, 'classifier'):\n",
    "            return out, self.classifier(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class IBGNN(torch.nn.Module):\n",
    "    def __init__(self, gnn, mlp, discriminator=lambda x, y: x @ y.t(), pooling='concat'):\n",
    "        super(IBGNN, self).__init__()\n",
    "        self.gnn = gnn\n",
    "        self.mlp = mlp\n",
    "        self.pooling = pooling\n",
    "        self.discriminator = discriminator\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch, edge_flag = data.x, data.edge_index, data.edge_attr, data.batch, data.edge_flag\n",
    "        g = self.gnn(x, edge_index, edge_attr, edge_flag, batch)\n",
    "        if self.pooling == 'concat':\n",
    "            _, g = self.mlp(g)\n",
    "            log_logits = F.log_softmax(g, dim=-1)\n",
    "            return log_logits\n",
    "        return g\n",
    "\n",
    "\n",
    "def build_model(device, num_features):\n",
    "    model = IBGNN(IBGConv(num_features, num_classes=2),\n",
    "                  MLP(16, 16, 1, torch.nn.ReLU, n_classes=2),\n",
    "                  pooling='sum').to(device)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nasdata4/pei4/anaconda3/envs/mye_venv/lib/python3.8/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "node_feature_type   = 'bold'\n",
    "edge_sparse_type    = 5\n",
    "snp_data_type       = 'real'\n",
    "QT_type             = 'norm'\n",
    "data_path           = '/nasdata4/pei4/feature_selection_gnn/sample_data/'\n",
    "\n",
    "dataset = my_dataset(\n",
    "    data_path, node_feature_type=node_feature_type, edge_sparse_type=edge_sparse_type, snp_data_type=snp_data_type\n",
    ")\n",
    "num_features    = dataset.train_dataset[0][0].x.shape[1]\n",
    "# define hyper parameter\n",
    "learning_rate   = 0.0001\n",
    "epochs          = 1\n",
    "batch_size      = 1\n",
    "\n",
    "train_loader    = DataLoader(dataset.train_dataset, batch_size=batch_size, drop_last=True)\n",
    "net             = build_model(device, num_features)\n",
    "criterion       = nn.MSELoss()\n",
    "optimizer       = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13456])\n"
     ]
    }
   ],
   "source": [
    "temp = dataset.train_dataset[0][0].edge_flag\n",
    "print(temp.shape)\n",
    "# for i in temp:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "3\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [672] at index 0 does not match the shape of the indexed tensor [13456] at index 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/nasdata4/pei4/feature_selection_gnn/test.ipynb 셀 11\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B210.117.210.90/nasdata4/pei4/feature_selection_gnn/test.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(data))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B210.117.210.90/nasdata4/pei4/feature_selection_gnn/test.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(data))\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B210.117.210.90/nasdata4/pei4/feature_selection_gnn/test.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m train_output \u001b[39m=\u001b[39m net(data[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B210.117.210.90/nasdata4/pei4/feature_selection_gnn/test.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m train_loss \u001b[39m=\u001b[39m criterion(train_output, data)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B210.117.210.90/nasdata4/pei4/feature_selection_gnn/test.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/nasdata4/pei4/anaconda3/envs/mye_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/nasdata4/pei4/feature_selection_gnn/test.ipynb 셀 11\u001b[0m in \u001b[0;36mIBGNN.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B210.117.210.90/nasdata4/pei4/feature_selection_gnn/test.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=176'>177</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, data):\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B210.117.210.90/nasdata4/pei4/feature_selection_gnn/test.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=177'>178</a>\u001b[0m     x, edge_index, edge_attr, batch, edge_flag \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mx, data\u001b[39m.\u001b[39medge_index, data\u001b[39m.\u001b[39medge_attr, data\u001b[39m.\u001b[39mbatch, data\u001b[39m.\u001b[39medge_flag\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B210.117.210.90/nasdata4/pei4/feature_selection_gnn/test.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=178'>179</a>\u001b[0m     g \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgnn(x, edge_index, edge_attr, edge_flag, batch)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B210.117.210.90/nasdata4/pei4/feature_selection_gnn/test.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=179'>180</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooling \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mconcat\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B210.117.210.90/nasdata4/pei4/feature_selection_gnn/test.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=180'>181</a>\u001b[0m         _, g \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(g)\n",
      "File \u001b[0;32m/nasdata4/pei4/anaconda3/envs/mye_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/nasdata4/pei4/feature_selection_gnn/test.ipynb 셀 11\u001b[0m in \u001b[0;36mIBGConv.forward\u001b[0;34m(self, x, edge_index, edge_attr, edge_flag, batch)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B210.117.210.90/nasdata4/pei4/feature_selection_gnn/test.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=130'>131</a>\u001b[0m edge_attr[edge_attr \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m edge_attr[edge_attr \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B210.117.210.90/nasdata4/pei4/feature_selection_gnn/test.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=131'>132</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, conv \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvs):\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B210.117.210.90/nasdata4/pei4/feature_selection_gnn/test.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=132'>133</a>\u001b[0m     z \u001b[39m=\u001b[39m conv(z, edge_index, edge_attr, edge_flag)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B210.117.210.90/nasdata4/pei4/feature_selection_gnn/test.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=133'>134</a>\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvs) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B210.117.210.90/nasdata4/pei4/feature_selection_gnn/test.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=134'>135</a>\u001b[0m         z \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(z)  \u001b[39m# [N * M, F]\u001b[39;00m\n",
      "File \u001b[0;32m/nasdata4/pei4/anaconda3/envs/mye_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/nasdata4/pei4/feature_selection_gnn/test.ipynb 셀 11\u001b[0m in \u001b[0;36mMPConv.forward\u001b[0;34m(self, x, edge_index, edge_attr, edge_flag)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B210.117.210.90/nasdata4/pei4/feature_selection_gnn/test.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=73'>74</a>\u001b[0m cache \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_edge_index\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B210.117.210.90/nasdata4/pei4/feature_selection_gnn/test.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39mif\u001b[39;00m cache \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B210.117.210.90/nasdata4/pei4/feature_selection_gnn/test.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=75'>76</a>\u001b[0m     edge_index, edge_weight, edge_flag \u001b[39m=\u001b[39m gcn_norm(  \u001b[39m# yapf: disable\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B210.117.210.90/nasdata4/pei4/feature_selection_gnn/test.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=76'>77</a>\u001b[0m         edge_index, edge_flag, edge_attr, x\u001b[39m.\u001b[39;49msize(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnode_dim),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B210.117.210.90/nasdata4/pei4/feature_selection_gnn/test.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimproved, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_self_loops)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B210.117.210.90/nasdata4/pei4/feature_selection_gnn/test.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcached:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B210.117.210.90/nasdata4/pei4/feature_selection_gnn/test.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_edge_index \u001b[39m=\u001b[39m (edge_index, edge_weight, edge_flag)\n",
      "\u001b[1;32m/nasdata4/pei4/feature_selection_gnn/test.ipynb 셀 11\u001b[0m in \u001b[0;36mgcn_norm\u001b[0;34m(edge_index, edge_flag, edge_attr, num_nodes, improved, do_add_self_loops, dtype)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B210.117.210.90/nasdata4/pei4/feature_selection_gnn/test.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(edge_index, Tensor):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B210.117.210.90/nasdata4/pei4/feature_selection_gnn/test.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(edge_flag, Tensor):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B210.117.210.90/nasdata4/pei4/feature_selection_gnn/test.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m         edge_index, edge_attr, edge_flag \u001b[39m=\u001b[39m _remove_self_loops(edge_index, edge_attr, edge_flag)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B210.117.210.90/nasdata4/pei4/feature_selection_gnn/test.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m         edge_index, edge_attr, edge_flag \u001b[39m=\u001b[39m _add_self_loops(edge_index, edge_attr, edge_flag,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B210.117.210.90/nasdata4/pei4/feature_selection_gnn/test.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m                                                            num_nodes\u001b[39m=\u001b[39mnum_nodes)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B210.117.210.90/nasdata4/pei4/feature_selection_gnn/test.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;32m/nasdata4/pei4/feature_selection_gnn/test.ipynb 셀 11\u001b[0m in \u001b[0;36m_remove_self_loops\u001b[0;34m(edge_index, edge_attr, edge_flags)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B210.117.210.90/nasdata4/pei4/feature_selection_gnn/test.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m mask \u001b[39m=\u001b[39m edge_index[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m edge_index[\u001b[39m1\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B210.117.210.90/nasdata4/pei4/feature_selection_gnn/test.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m edge_index \u001b[39m=\u001b[39m edge_index[:, mask]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B210.117.210.90/nasdata4/pei4/feature_selection_gnn/test.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mreturn\u001b[39;00m edge_index, edge_attr[mask], edge_flags[mask]\n",
      "\u001b[0;31mIndexError\u001b[0m: The shape of the mask [672] at index 0 does not match the shape of the indexed tensor [13456] at index 0"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    net.train()\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        print(type(data))\n",
    "        print(len(data))\n",
    "        train_output = net(data[0])\n",
    "        train_loss = criterion(train_output, data)\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "    # print message\n",
    "    epoch_len = len(str(epochs))\n",
    "    print_msg = (f\"[{epoch:>{epoch_len}}/{epochs:>{epoch_len}}] {train_loss}\")\n",
    "    print(print_msg)\n",
    "\n",
    "print(\"Finished Training\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.8823, 0.9150, 0.3829, 0.9593, 0.3904]],\n",
      "\n",
      "        [[0.6009, 0.2566, 0.7936, 0.9408, 0.1332]],\n",
      "\n",
      "        [[0.9346, 0.5936, 0.8694, 0.5677, 0.7411]],\n",
      "\n",
      "        [[0.4294, 0.8854, 0.5739, 0.2666, 0.6274]],\n",
      "\n",
      "        [[0.2696, 0.4414, 0.2969, 0.8317, 0.1053]],\n",
      "\n",
      "        [[0.2695, 0.3588, 0.1994, 0.5472, 0.0062]],\n",
      "\n",
      "        [[0.9516, 0.0753, 0.8860, 0.5832, 0.3376]],\n",
      "\n",
      "        [[0.8090, 0.5779, 0.9040, 0.5547, 0.3423]],\n",
      "\n",
      "        [[0.6343, 0.3644, 0.7104, 0.9464, 0.7890]],\n",
      "\n",
      "        [[0.2814, 0.7886, 0.5895, 0.7539, 0.1952]],\n",
      "\n",
      "        [[0.0050, 0.3068, 0.1165, 0.9103, 0.6440]],\n",
      "\n",
      "        [[0.7071, 0.6581, 0.4913, 0.8913, 0.1447]],\n",
      "\n",
      "        [[0.5315, 0.1587, 0.6542, 0.3278, 0.6532]],\n",
      "\n",
      "        [[0.3958, 0.9147, 0.2036, 0.2018, 0.2018]],\n",
      "\n",
      "        [[0.9497, 0.6666, 0.9811, 0.0874, 0.0041]],\n",
      "\n",
      "        [[0.1088, 0.1637, 0.7025, 0.6790, 0.9155]]])\n",
      "tensor([0.2418, 0.1591, 0.7653, 0.2979, 0.8035])\n",
      "\n",
      "==>> catted_edge_mask:  torch.Size([80])\n",
      "tensor([0.2418, 0.1591, 0.7653, 0.2979, 0.8035, 0.2418, 0.1591, 0.7653, 0.2979,\n",
      "        0.8035, 0.2418, 0.1591, 0.7653, 0.2979, 0.8035, 0.2418, 0.1591, 0.7653,\n",
      "        0.2979, 0.8035, 0.2418, 0.1591, 0.7653, 0.2979, 0.8035, 0.2418, 0.1591,\n",
      "        0.7653, 0.2979, 0.8035, 0.2418, 0.1591, 0.7653, 0.2979, 0.8035, 0.2418,\n",
      "        0.1591, 0.7653, 0.2979, 0.8035, 0.2418, 0.1591, 0.7653, 0.2979, 0.8035,\n",
      "        0.2418, 0.1591, 0.7653, 0.2979, 0.8035, 0.2418, 0.1591, 0.7653, 0.2979,\n",
      "        0.8035, 0.2418, 0.1591, 0.7653, 0.2979, 0.8035, 0.2418, 0.1591, 0.7653,\n",
      "        0.2979, 0.8035, 0.2418, 0.1591, 0.7653, 0.2979, 0.8035, 0.2418, 0.1591,\n",
      "        0.7653, 0.2979, 0.8035, 0.2418, 0.1591, 0.7653, 0.2979, 0.8035])\n",
      "==>> iter edge_flag:  16\n",
      "[tensor([0.8823, 0.9150, 0.3829, 0.9593, 0.3904]), tensor([0.6009, 0.2566, 0.7936, 0.9408, 0.1332]), tensor([0.9346, 0.5936, 0.8694, 0.5677, 0.7411]), tensor([0.4294, 0.8854, 0.5739, 0.2666, 0.6274]), tensor([0.2696, 0.4414, 0.2969, 0.8317, 0.1053]), tensor([0.2695, 0.3588, 0.1994, 0.5472, 0.0062]), tensor([0.9516, 0.0753, 0.8860, 0.5832, 0.3376]), tensor([0.8090, 0.5779, 0.9040, 0.5547, 0.3423]), tensor([0.6343, 0.3644, 0.7104, 0.9464, 0.7890]), tensor([0.2814, 0.7886, 0.5895, 0.7539, 0.1952]), tensor([0.0050, 0.3068, 0.1165, 0.9103, 0.6440]), tensor([0.7071, 0.6581, 0.4913, 0.8913, 0.1447]), tensor([0.5315, 0.1587, 0.6542, 0.3278, 0.6532]), tensor([0.3958, 0.9147, 0.2036, 0.2018, 0.2018]), tensor([0.9497, 0.6666, 0.9811, 0.0874, 0.0041]), tensor([0.1088, 0.1637, 0.7025, 0.6790, 0.9155])]\n",
      "==>> concat edge_flag:  (80,)\n",
      "[0.88226926 0.91500396 0.38286376 0.95930564 0.3904482  0.60089535\n",
      " 0.25657248 0.7936413  0.94077146 0.13318592 0.9345981  0.59357965\n",
      " 0.86940444 0.5677153  0.74109405 0.4294045  0.8854429  0.57390445\n",
      " 0.26658005 0.62744915 0.26963168 0.44136357 0.29692084 0.8316855\n",
      " 0.10531491 0.26949483 0.35881263 0.19936377 0.54719156 0.00616044\n",
      " 0.95155454 0.07526588 0.8860137  0.5832096  0.33764774 0.808975\n",
      " 0.5779254  0.9039817  0.55465984 0.3423134  0.63434184 0.36441028\n",
      " 0.7104288  0.9464111  0.7890298  0.28141373 0.78863233 0.5894631\n",
      " 0.7539175  0.19524747 0.00504577 0.30681974 0.11648858 0.91026944\n",
      " 0.64401567 0.70710677 0.6581306  0.491302   0.89130414 0.1447432\n",
      " 0.53148186 0.15872991 0.654176   0.32780886 0.65320814 0.39582926\n",
      " 0.9146959  0.20364904 0.201801   0.201783   0.9497214  0.66662556\n",
      " 0.98112535 0.08736187 0.00406194 0.10881811 0.16365546 0.7025201\n",
      " 0.6790379  0.9154622 ]\n",
      "==>> pruned_edge_mask:  torch.Size([80])\n",
      "tensor([0.2418, 0.2418, 0.2418, 0.2418, 0.2418, 0.2418, 0.2418, 0.2418, 0.2418,\n",
      "        0.2418, 0.2418, 0.2418, 0.2418, 0.2418, 0.2418, 0.2418, 0.2418, 0.2418,\n",
      "        0.2418, 0.2418, 0.2418, 0.2418, 0.2418, 0.2418, 0.2418, 0.2418, 0.2418,\n",
      "        0.2418, 0.2418, 0.2418, 0.2418, 0.2418, 0.2418, 0.2418, 0.2418, 0.2418,\n",
      "        0.2418, 0.2418, 0.2418, 0.2418, 0.2418, 0.2418, 0.2418, 0.2418, 0.2418,\n",
      "        0.2418, 0.2418, 0.2418, 0.2418, 0.2418, 0.2418, 0.2418, 0.2418, 0.2418,\n",
      "        0.2418, 0.2418, 0.2418, 0.2418, 0.2418, 0.2418, 0.2418, 0.2418, 0.2418,\n",
      "        0.2418, 0.2418, 0.2418, 0.2418, 0.2418, 0.2418, 0.2418, 0.2418, 0.2418,\n",
      "        0.2418, 0.2418, 0.2418, 0.2418, 0.2418, 0.2418, 0.2418, 0.2418])\n",
      "tensor([0.2418, 0.1591, 0.7653, 0.2979, 0.8035, 0.2418, 0.1591, 0.7653, 0.2979,\n",
      "        0.8035, 0.2418, 0.1591, 0.7653, 0.2979, 0.8035, 0.2418, 0.1591, 0.7653,\n",
      "        0.2979, 0.8035, 0.2418, 0.1591, 0.7653, 0.2979, 0.8035, 0.2418, 0.1591,\n",
      "        0.7653, 0.2979, 0.8035, 0.2418, 0.1591, 0.7653, 0.2979, 0.8035, 0.2418,\n",
      "        0.1591, 0.7653, 0.2979, 0.8035, 0.2418, 0.1591, 0.7653, 0.2979, 0.8035,\n",
      "        0.2418, 0.1591, 0.7653, 0.2979, 0.8035, 0.2418, 0.1591, 0.7653, 0.2979,\n",
      "        0.8035, 0.2418, 0.1591, 0.7653, 0.2979, 0.8035, 0.2418, 0.1591, 0.7653,\n",
      "        0.2979, 0.8035, 0.2418, 0.1591, 0.7653, 0.2979, 0.8035, 0.2418, 0.1591,\n",
      "        0.7653, 0.2979, 0.8035, 0.2418, 0.1591, 0.7653, 0.2979, 0.8035])\n"
     ]
    }
   ],
   "source": [
    "edge_flag = torch.rand(size=(16, 1, 5))\n",
    "edge_mask = torch.rand(size=(1, 5))\n",
    "edge_mask = edge_mask.squeeze(0)\n",
    "print(edge_flag)\n",
    "print(edge_mask)\n",
    "print()\n",
    "catted_edge_mask = torch.cat(len(edge_flag) * [edge_mask]) # -> edge flag 에 동일하게 마스크 씌워주기 위해서 edge\n",
    "print(\"==>> catted_edge_mask: \", catted_edge_mask.shape)\n",
    "print(catted_edge_mask)\n",
    "if (len(edge_flag) != 1):\n",
    "    edge_flag = [i[0] for i in edge_flag]\n",
    "    # print(\"==>> edge_flag: \", edge_flag)\n",
    "    print(\"==>> iter edge_flag: \", len(edge_flag))\n",
    "    print(edge_flag)\n",
    "    edge_flag = np.concatenate(edge_flag)\n",
    "    print(\"==>> concat edge_flag: \", edge_flag.shape)\n",
    "    print(edge_flag)\n",
    "pruned_edge_mask = catted_edge_mask[edge_flag]\n",
    "print(\"==>> pruned_edge_mask: \", pruned_edge_mask.shape)\n",
    "print(pruned_edge_mask)\n",
    "print(catted_edge_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> term_1: \n",
      " [[0.10843137 0.139794   0.1       ]\n",
      " [0.139794   0.10843137 0.1       ]\n",
      " [0.1        0.139794   0.10843137]]\n",
      "\n",
      "==>> term_2: \n",
      " [[-0.15686362 -0.07752801 -0.04118174]\n",
      " [-0.07752801 -0.15686362 -0.04118174]\n",
      " [-0.04118174 -0.07752801 -0.15686362]]\n",
      "\n",
      "loss : \n",
      " [[-0.04843225  0.06226599  0.05881826]\n",
      " [ 0.06226599 -0.04843225  0.05881826]\n",
      " [ 0.05881826  0.06226599 -0.04843225]]\n",
      "\n",
      "==>> term_1: \n",
      " [[0.07752801 0.1        0.1       ]\n",
      " [0.1        0.07752801 0.1       ]\n",
      " [0.1        0.1        0.07752801]]\n",
      "\n",
      "==>> term_2: \n",
      " [[-0.139794   -0.04118174 -0.04118174]\n",
      " [-0.04118174 -0.139794   -0.04118174]\n",
      " [-0.04118174 -0.04118174 -0.139794  ]]\n",
      "\n",
      "loss : \n",
      " [[-0.06226599  0.05881826  0.05881826]\n",
      " [ 0.05881826 -0.06226599  0.05881826]\n",
      " [ 0.05881826  0.05881826 -0.06226599]]\n",
      "\n",
      "==>> term_1: \n",
      " [[-4.34294518e-11 -0.00000000e+00 -0.00000000e+00]\n",
      " [-0.00000000e+00 -4.34294518e-11 -0.00000000e+00]\n",
      " [-0.00000000e+00 -0.00000000e+00 -4.34294518e-11]]\n",
      "\n",
      "==>> term_2: \n",
      " [[-0.00000000e+00  4.34294518e-11  4.34294518e-11]\n",
      " [ 4.34294518e-11 -0.00000000e+00  4.34294518e-11]\n",
      " [ 4.34294518e-11  4.34294518e-11 -0.00000000e+00]]\n",
      "\n",
      "loss : \n",
      " [[-4.34294518e-11  4.34294518e-11  4.34294518e-11]\n",
      " [ 4.34294518e-11 -4.34294518e-11  4.34294518e-11]\n",
      " [ 4.34294518e-11  4.34294518e-11 -4.34294518e-11]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "M1 = np.asarray([[0.7, 0.2, 0.1],\n",
    "                [0.2, 0.7, 0.1],\n",
    "                [0.1, 0.2, 0.7]])\n",
    "\n",
    "M2 = np.asarray([[0.8, 0.1, 0.1],\n",
    "                [0.1, 0.8, 0.1],\n",
    "                [0.1, 0.1, 0.8]])\n",
    "\n",
    "M3 = np.asarray([[1, 0, 0],\n",
    "                [0, 1, 0],\n",
    "                [0, 0, 1]])\n",
    "\n",
    "def loss_test(M):\n",
    "    EPS = 1E-10\n",
    "    term_1 = -1 * M * np.log10(M + EPS)\n",
    "    term_2 = (1-M) * np.log10(1-M + EPS)\n",
    "    print(\"==>> term_1: \\n\", term_1)\n",
    "    print()\n",
    "    print(\"==>> term_2: \\n\", term_2)\n",
    "    print()\n",
    "    print(\"loss : \\n\", term_1 + term_2)\n",
    "    print()\n",
    "\n",
    "loss_test(M1)\n",
    "loss_test(M2)\n",
    "loss_test(M3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "M1 = np.asarray([[0.7, 0.2, 0.1],\n",
    "                [0.2, 0.7, 0.1],\n",
    "                [0.1, 0.2, 0.7]])\n",
    "\n",
    "for i in range(0, 100):\n",
    "    M1 = sigmoid(M1)\n",
    "\n",
    "print(M1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(100).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('dcm': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "e14f7b4794afc15796f74972c1ba2983c18a0c12e632b66368613e54d9126cbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
